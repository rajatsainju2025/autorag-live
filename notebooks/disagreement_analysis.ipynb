{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d28d16",
   "metadata": {},
   "source": [
    "# Disagreement Analysis in RAG Systems\n",
    "\n",
    "This notebook demonstrates how to analyze disagreements between different retrievers in a Retrieval-Augmented Generation (RAG) system using the `autorag-live` library.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Disagreement analysis helps understand:\n",
    "- How different retrievers rank the same documents\n",
    "- Which retrievers are most consistent\n",
    "- Where retrievers disagree and why\n",
    "- How to optimize hybrid retriever weights\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25499846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install autorag-live sentence-transformers scipy\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Import autorag-live components\n",
    "from autorag_live.retrievers import bm25, dense, hybrid\n",
    "from autorag_live.disagreement import metrics, report\n",
    "from autorag_live.pipeline.hybrid_optimizer import grid_search_hybrid_weights\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e58646",
   "metadata": {},
   "source": [
    "## Sample Data\n",
    "\n",
    "Let's create a small corpus of documents to demonstrate disagreement analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc0bb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample corpus for demonstration\n",
    "CORPUS = [\n",
    "    \"The sky is blue and beautiful during the day.\",\n",
    "    \"The sun rises in the east and sets in the west.\",\n",
    "    \"The sun is bright and provides light to Earth.\",\n",
    "    \"The sun in the sky is very bright during daytime.\",\n",
    "    \"We can see the shining sun, the bright sun in the sky.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"A lazy fox is usually sleeping in its den.\",\n",
    "    \"The fox is a mammal that belongs to the canine family.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Deep learning uses neural networks with multiple layers.\",\n",
    "    \"Natural language processing helps computers understand text.\",\n",
    "    \"Computer vision enables machines to interpret visual information.\",\n",
    "    \"Data science combines statistics, programming, and domain expertise.\",\n",
    "    \"Python is a popular programming language for data science.\",\n",
    "    \"Jupyter notebooks provide an interactive environment for coding.\"\n",
    "]\n",
    "\n",
    "QUERIES = [\n",
    "    \"bright sun in the sky\",\n",
    "    \"fox jumping over dog\",\n",
    "    \"machine learning and AI\",\n",
    "    \"programming with Python\"\n",
    "]\n",
    "\n",
    "print(f\"Corpus size: {len(CORPUS)} documents\")\n",
    "print(f\"Number of queries: {len(QUERIES)}\")\n",
    "print(\"\\nSample documents:\")\n",
    "for i, doc in enumerate(CORPUS[:3]):\n",
    "    print(f\"{i+1}. {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83529756",
   "metadata": {},
   "source": [
    "## Retriever Setup\n",
    "\n",
    "Let's initialize different retrievers and see how they perform on our queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8faeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize retrievers\n",
    "print(\"Initializing retrievers...\")\n",
    "\n",
    "# BM25 retriever (lexical)\n",
    "bm25_retriever = bm25\n",
    "\n",
    "# Dense retriever (semantic)\n",
    "dense_retriever = dense\n",
    "\n",
    "# Hybrid retriever (combination)\n",
    "hybrid_retriever = hybrid\n",
    "\n",
    "print(\"✅ Retrievers initialized\")\n",
    "\n",
    "# Test each retriever on a sample query\n",
    "test_query = QUERIES[0]\n",
    "k = 5\n",
    "\n",
    "print(f\"\\nTesting retrievers on query: '{test_query}'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get results from each retriever\n",
    "bm25_results = bm25_retriever.bm25_retrieve(test_query, CORPUS, k)\n",
    "dense_results = dense_retriever.dense_retrieve(test_query, CORPUS, k)\n",
    "hybrid_results = hybrid_retriever.hybrid_retrieve(test_query, CORPUS, k)\n",
    "\n",
    "print(\"BM25 Results:\")\n",
    "for i, doc in enumerate(bm25_results):\n",
    "    print(f\"{i+1}. {doc}\")\n",
    "\n",
    "print(\"\\nDense Results:\")\n",
    "for i, doc in enumerate(dense_results):\n",
    "    print(f\"{i+1}. {doc}\")\n",
    "\n",
    "print(\"\\nHybrid Results:\")\n",
    "for i, doc in enumerate(hybrid_results):\n",
    "    print(f\"{i+1}. {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8042f926",
   "metadata": {},
   "source": [
    "## Disagreement Analysis\n",
    "\n",
    "Now let's analyze the disagreements between different retrievers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9c2e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_disagreements(query: str, corpus: List[str], k: int = 5):\n",
    "    \"\"\"Analyze disagreements between retrievers for a given query.\"\"\"\n",
    "    \n",
    "    # Get results from all retrievers\n",
    "    bm25_results = bm25_retriever.bm25_retrieve(query, corpus, k)\n",
    "    dense_results = dense_retriever.dense_retrieve(query, corpus, k)\n",
    "    hybrid_results = hybrid_retriever.hybrid_retrieve(query, corpus, k)\n",
    "    \n",
    "    results = {\n",
    "        \"BM25\": bm25_results,\n",
    "        \"Dense\": dense_results,\n",
    "        \"Hybrid\": hybrid_results\n",
    "    }\n",
    "    \n",
    "    # Calculate disagreement metrics\n",
    "    disagreement_metrics = {\n",
    "        \"jaccard_bm25_vs_dense\": metrics.jaccard_at_k(bm25_results, dense_results),\n",
    "        \"kendall_tau_bm25_vs_dense\": metrics.kendall_tau_at_k(bm25_results, dense_results),\n",
    "        \"jaccard_bm25_vs_hybrid\": metrics.jaccard_at_k(bm25_results, hybrid_results),\n",
    "        \"kendall_tau_bm25_vs_hybrid\": metrics.kendall_tau_at_k(bm25_results, hybrid_results),\n",
    "        \"jaccard_dense_vs_hybrid\": metrics.jaccard_at_k(dense_results, hybrid_results),\n",
    "        \"kendall_tau_dense_vs_hybrid\": metrics.kendall_tau_at_k(dense_results, hybrid_results),\n",
    "    }\n",
    "    \n",
    "    return results, disagreement_metrics\n",
    "\n",
    "# Analyze disagreements for all queries\n",
    "all_results = {}\n",
    "all_metrics = {}\n",
    "\n",
    "print(\"Analyzing disagreements across all queries...\")\n",
    "for query in QUERIES:\n",
    "    results, disagreement_metrics = analyze_disagreements(query, CORPUS)\n",
    "    all_results[query] = results\n",
    "    all_metrics[query] = disagreement_metrics\n",
    "    \n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(\"-\" * 40)\n",
    "    for metric_name, value in disagreement_metrics.items():\n",
    "        print(f\"{metric_name}: {value:.3f}\")\n",
    "\n",
    "print(\"\\n✅ Disagreement analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16efc5ed",
   "metadata": {},
   "source": [
    "## Visualization of Disagreements\n",
    "\n",
    "Let's create some visualizations to better understand the disagreements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19a5b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary DataFrame of disagreement metrics\n",
    "metrics_data = []\n",
    "for query, metrics_dict in all_metrics.items():\n",
    "    for metric_name, value in metrics_dict.items():\n",
    "        retriever_pair = metric_name.split('_')[-1].replace('_', ' vs ')\n",
    "        metric_type = 'Jaccard' if 'jaccard' in metric_name else 'Kendall Tau'\n",
    "        metrics_data.append({\n",
    "            'Query': query,\n",
    "            'Retriever Pair': retriever_pair,\n",
    "            'Metric': metric_type,\n",
    "            'Value': value\n",
    "        })\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Plot disagreement metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Disagreement Analysis Across Queries', fontsize=16)\n",
    "\n",
    "# Jaccard similarity heatmap\n",
    "jaccard_data = df_metrics[df_metrics['Metric'] == 'Jaccard'].pivot(\n",
    "    index='Query', columns='Retriever Pair', values='Value'\n",
    ")\n",
    "sns.heatmap(jaccard_data, annot=True, cmap='YlOrRd', ax=axes[0,0])\n",
    "axes[0,0].set_title('Jaccard Similarity (Higher = More Agreement)')\n",
    "\n",
    "# Kendall Tau correlation heatmap\n",
    "kendall_data = df_metrics[df_metrics['Metric'] == 'Kendall Tau'].pivot(\n",
    "    index='Query', columns='Retriever Pair', values='Value'\n",
    ")\n",
    "sns.heatmap(kendall_data, annot=True, cmap='RdYlBu_r', ax=axes[0,1])\n",
    "axes[0,1].set_title('Kendall Tau Correlation (Higher = More Agreement)')\n",
    "\n",
    "# Average metrics by retriever pair\n",
    "avg_metrics = df_metrics.groupby(['Retriever Pair', 'Metric'])['Value'].mean().reset_index()\n",
    "sns.barplot(data=avg_metrics, x='Retriever Pair', y='Value', hue='Metric', ax=axes[1,0])\n",
    "axes[1,0].set_title('Average Agreement by Retriever Pair')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Distribution of disagreement metrics\n",
    "sns.boxplot(data=df_metrics, x='Metric', y='Value', ax=axes[1,1])\n",
    "axes[1,1].set_title('Distribution of Disagreement Metrics')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "print(df_metrics.groupby(['Metric', 'Retriever Pair'])['Value'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea08a61",
   "metadata": {},
   "source": [
    "## Generate HTML Report\n",
    "\n",
    "Let's generate a comprehensive HTML report of our disagreement analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8542ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate HTML report for the first query\n",
    "test_query = QUERIES[0]\n",
    "results = all_results[test_query]\n",
    "disagreement_metrics = all_metrics[test_query]\n",
    "\n",
    "# Ensure reports directory exists\n",
    "os.makedirs('reports', exist_ok=True)\n",
    "\n",
    "# Generate the report\n",
    "report_path = f\"reports/disagreement_analysis_{test_query.replace(' ', '_')}.html\"\n",
    "report.generate_disagreement_report(\n",
    "    test_query, \n",
    "    results, \n",
    "    disagreement_metrics, \n",
    "    report_path\n",
    ")\n",
    "\n",
    "print(f\"✅ HTML report generated: {report_path}\")\n",
    "print(\"\\nOpen the HTML file in your browser to view the detailed report.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfd8631",
   "metadata": {},
   "source": [
    "## Hybrid Weight Optimization\n",
    "\n",
    "Based on our disagreement analysis, let's optimize the hybrid retriever weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eed524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize hybrid weights using grid search\n",
    "print(\"Optimizing hybrid retriever weights...\")\n",
    "\n",
    "# Use a subset of queries for optimization\n",
    "optimization_queries = QUERIES[:2]  # Use first 2 queries\n",
    "\n",
    "# Perform grid search\n",
    "optimal_weights, best_score = grid_search_hybrid_weights(\n",
    "    optimization_queries, \n",
    "    CORPUS, \n",
    "    k=5, \n",
    "    grid_size=5  # Try 5x5 grid of weight combinations\n",
    ")\n",
    "\n",
    "print(f\"\\nOptimal weights found:\")\n",
    "print(f\"BM25 weight: {optimal_weights.bm25_weight:.3f}\")\n",
    "print(f\"Dense weight: {optimal_weights.dense_weight:.3f}\")\n",
    "print(f\"Best diversity score: {best_score:.3f}\")\n",
    "\n",
    "# Test the optimized hybrid retriever\n",
    "print(\"\\nTesting optimized hybrid retriever:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Update hybrid weights (this would normally be saved to config)\n",
    "from autorag_live.pipeline.hybrid_optimizer import save_hybrid_config\n",
    "save_hybrid_config(optimal_weights)\n",
    "\n",
    "# Test on a new query\n",
    "test_results = hybrid.hybrid_retrieve(QUERIES[2], CORPUS, 5)\n",
    "print(f\"Results for query '{QUERIES[2]}':\")\n",
    "for i, doc in enumerate(test_results):\n",
    "    print(f\"{i+1}. {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03871b05",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "From this disagreement analysis, we can observe:\n",
    "\n",
    "1. **Agreement Patterns**: Different retriever pairs show varying levels of agreement\n",
    "2. **Query Sensitivity**: Some queries show more disagreement than others\n",
    "3. **Optimization Potential**: Hybrid approaches can leverage complementary strengths\n",
    "4. **Evaluation Metrics**: Jaccard and Kendall Tau provide different perspectives on agreement\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try different corpora and query types\n",
    "- Experiment with different retriever combinations\n",
    "- Implement more sophisticated optimization algorithms\n",
    "- Add more evaluation metrics and analysis techniques\n",
    "\n",
    "This notebook demonstrates the core disagreement analysis capabilities of the `autorag-live` system."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
