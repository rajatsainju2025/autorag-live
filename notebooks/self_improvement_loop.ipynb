{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4463ffe8",
   "metadata": {},
   "source": [
    "# Self-Improving RAG System\n",
    "\n",
    "This notebook demonstrates the self-improvement loop of the `autorag-live` system, where the RAG pipeline automatically optimizes itself through disagreement analysis and iterative refinement.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The self-improvement loop consists of:\n",
    "1. **Evaluation**: Run evaluation suites to measure current performance\n",
    "2. **Disagreement Analysis**: Analyze disagreements between retrievers\n",
    "3. **Optimization**: Optimize hybrid weights and other parameters\n",
    "4. **Acceptance Policy**: Decide whether to accept or reject changes\n",
    "5. **Iteration**: Repeat the process with improved configuration\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb07323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install autorag-live sentence-transformers scipy\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# Import autorag-live components\n",
    "from autorag_live.retrievers import bm25, dense, hybrid\n",
    "from autorag_live.disagreement import metrics\n",
    "from autorag_live.evals.small import run_small_suite\n",
    "from autorag_live.pipeline.hybrid_optimizer import (\n",
    "    grid_search_hybrid_weights, \n",
    "    save_hybrid_config,\n",
    "    load_hybrid_config\n",
    ")\n",
    "from autorag_live.pipeline.acceptance_policy import AcceptancePolicy\n",
    "from autorag_live.augment.synonym_miner import (\n",
    "    mine_synonyms_from_disagreements,\n",
    "    update_terms_from_mining\n",
    ")\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "import seaborn as sns\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53b945d",
   "metadata": {},
   "source": [
    "## Sample Data and Initial Setup\n",
    "\n",
    "Let's set up our test data and initial system configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3b7523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample corpus\n",
    "CORPUS = [\n",
    "    \"The sky is blue and beautiful during the day.\",\n",
    "    \"The sun rises in the east and sets in the west.\",\n",
    "    \"The sun is bright and provides light to Earth.\",\n",
    "    \"The sun in the sky is very bright during daytime.\",\n",
    "    \"We can see the shining sun, the bright sun in the sky.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"A lazy fox is usually sleeping in its den.\",\n",
    "    \"The fox is a mammal that belongs to the canine family.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Deep learning uses neural networks with multiple layers.\",\n",
    "    \"Natural language processing helps computers understand text.\",\n",
    "    \"Computer vision enables machines to interpret visual information.\",\n",
    "    \"Data science combines statistics, programming, and domain expertise.\",\n",
    "    \"Python is a popular programming language for data science.\",\n",
    "    \"Jupyter notebooks provide an interactive environment for coding.\"\n",
    "]\n",
    "\n",
    "# Evaluation queries (different from training queries)\n",
    "EVAL_QUERIES = [\n",
    "    \"bright sun in the sky\",\n",
    "    \"fox jumping over dog\", \n",
    "    \"machine learning and AI\",\n",
    "    \"programming with Python\",\n",
    "    \"data science techniques\",\n",
    "    \"natural language processing\",\n",
    "    \"computer vision applications\"\n",
    "]\n",
    "\n",
    "# Training queries for optimization\n",
    "TRAIN_QUERIES = [\n",
    "    \"sun bright sky\",\n",
    "    \"fox dog jump\",\n",
    "    \"AI machine learning\",\n",
    "    \"Python programming\"\n",
    "]\n",
    "\n",
    "print(f\"Corpus: {len(CORPUS)} documents\")\n",
    "print(f\"Training queries: {len(TRAIN_QUERIES)}\")\n",
    "print(f\"Evaluation queries: {len(EVAL_QUERIES)}\")\n",
    "\n",
    "# Ensure necessary directories exist\n",
    "os.makedirs('runs', exist_ok=True)\n",
    "os.makedirs('reports', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b29277",
   "metadata": {},
   "source": [
    "## Self-Improvement Loop Implementation\n",
    "\n",
    "Let's implement the core self-improvement loop that iteratively optimizes the RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873e3f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfImprovingRAG:\n",
    "    \"\"\"Self-improving RAG system with automatic optimization.\"\"\"\n",
    "    \n",
    "    def __init__(self, corpus: List[str], max_iterations: int = 5):\n",
    "        self.corpus = corpus\n",
    "        self.max_iterations = max_iterations\n",
    "        self.history = []\n",
    "        self.acceptance_policy = AcceptancePolicy(threshold=0.01)\n",
    "        \n",
    "        # Load current configuration\n",
    "        try:\n",
    "            self.current_config = load_hybrid_config()\n",
    "        except:\n",
    "            # Default configuration\n",
    "            self.current_config = type('Config', (), {\n",
    "                'bm25_weight': 0.5,\n",
    "                'dense_weight': 0.5\n",
    "            })()\n",
    "    \n",
    "    def evaluate_current_performance(self, queries: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate current system performance.\"\"\"\n",
    "        print(\"ðŸ“Š Evaluating current performance...\")\n",
    "        \n",
    "        # Run evaluation suite\n",
    "        summary = run_small_suite(judge_type=\"deterministic\")\n",
    "        \n",
    "        # Calculate average disagreement diversity\n",
    "        diversity_scores = []\n",
    "        for query in queries[:3]:  # Use subset for speed\n",
    "            bm25_results = bm25.bm25_retrieve(query, self.corpus, 5)\n",
    "            dense_results = dense.dense_retrieve(query, self.corpus, 5)\n",
    "            hybrid_results = hybrid.hybrid_retrieve(query, self.corpus, 5)\n",
    "            \n",
    "            # Calculate diversity as average disagreement\n",
    "            jaccard_bd = metrics.jaccard_at_k(bm25_results, dense_results)\n",
    "            jaccard_bh = metrics.jaccard_at_k(bm25_results, hybrid_results)\n",
    "            jaccard_dh = metrics.jaccard_at_k(dense_results, hybrid_results)\n",
    "            \n",
    "            avg_diversity = (jaccard_bd + jaccard_bh + jaccard_dh) / 3\n",
    "            diversity_scores.append(avg_diversity)\n",
    "        \n",
    "        avg_diversity = np.mean(diversity_scores)\n",
    "        \n",
    "        metrics_dict = {\n",
    "            'em': summary['metrics']['em'],\n",
    "            'f1': summary['metrics']['f1'],\n",
    "            'relevance': summary['metrics']['relevance'],\n",
    "            'faithfulness': summary['metrics']['faithfulness'],\n",
    "            'diversity': avg_diversity,\n",
    "            'run_id': summary['run_id']\n",
    "        }\n",
    "        \n",
    "        print(f\"   EM: {metrics_dict['em']:.3f}, F1: {metrics_dict['f1']:.3f}\")\n",
    "        print(f\"   Diversity: {metrics_dict['diversity']:.3f}\")\n",
    "        \n",
    "        return metrics_dict\n",
    "    \n",
    "    def optimize_hybrid_weights(self, train_queries: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Optimize hybrid retriever weights.\"\"\"\n",
    "        print(\"ðŸ”§ Optimizing hybrid weights...\")\n",
    "        \n",
    "        # Perform grid search\n",
    "        optimal_weights, best_score = grid_search_hybrid_weights(\n",
    "            train_queries, \n",
    "            self.corpus, \n",
    "            k=5, \n",
    "            grid_size=4\n",
    "        )\n",
    "        \n",
    "        print(f\"   New weights - BM25: {optimal_weights.bm25_weight:.3f}, \"\n",
    "              f\"Dense: {optimal_weights.dense_weight:.3f}\")\n",
    "        print(f\"   Diversity score: {best_score:.3f}\")\n",
    "        \n",
    "        return {\n",
    "            'weights': optimal_weights,\n",
    "            'score': best_score\n",
    "        }\n",
    "    \n",
    "    def mine_synonyms(self, queries: List[str]) -> int:\n",
    "        \"\"\"Mine synonyms from retriever disagreements.\"\"\"\n",
    "        print(\"ðŸ“š Mining synonyms from disagreements...\")\n",
    "        \n",
    "        mined_synonyms = []\n",
    "        for query in queries[:2]:  # Use subset for speed\n",
    "            bm25_results = bm25.bm25_retrieve(query, self.corpus, 5)\n",
    "            dense_results = dense.dense_retrieve(query, self.corpus, 5)\n",
    "            hybrid_results = hybrid.hybrid_retrieve(query, self.corpus, 5)\n",
    "            \n",
    "            synonyms = mine_synonyms_from_disagreements(\n",
    "                bm25_results, dense_results, hybrid_results\n",
    "            )\n",
    "            mined_synonyms.extend(synonyms)\n",
    "        \n",
    "        if mined_synonyms:\n",
    "            update_terms_from_mining(mined_synonyms)\n",
    "            print(f\"   Added {len(mined_synonyms)} synonym groups\")\n",
    "        else:\n",
    "            print(\"   No new synonyms found\")\n",
    "        \n",
    "        return len(mined_synonyms)\n",
    "    \n",
    "    def apply_optimization(self, optimization_result: Dict[str, Any]) -> bool:\n",
    "        \"\"\"Apply optimization results with acceptance policy.\"\"\"\n",
    "        print(\"âœ… Applying optimization...\")\n",
    "        \n",
    "        def update_func():\n",
    "            save_hybrid_config(optimization_result['weights'])\n",
    "            self.current_config = optimization_result['weights']\n",
    "        \n",
    "        # Use acceptance policy to decide whether to apply changes\n",
    "        accepted = self.acceptance_policy.safe_update(\n",
    "            update_func, \n",
    "            [\"hybrid_config.json\"],\n",
    "            expected_improvement=optimization_result['score'] - 0.5  # Baseline diversity\n",
    "        )\n",
    "        \n",
    "        if accepted:\n",
    "            print(\"   âœ… Optimization accepted\")\n",
    "        else:\n",
    "            print(\"   âŒ Optimization rejected (reverted)\")\n",
    "        \n",
    "        return accepted\n",
    "    \n",
    "    def run_improvement_loop(self, train_queries: List[str], eval_queries: List[str]):\n",
    "        \"\"\"Run the complete self-improvement loop.\"\"\"\n",
    "        print(\"ðŸš€ Starting Self-Improvement Loop\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for iteration in range(self.max_iterations):\n",
    "            print(f\"\\nðŸ”„ Iteration {iteration + 1}/{self.max_iterations}\")\n",
    "            print(\"-\" * 30)\n",
    "            \n",
    "            # Step 1: Evaluate current performance\n",
    "            current_metrics = self.evaluate_current_performance(eval_queries)\n",
    "            \n",
    "            # Step 2: Mine synonyms from disagreements\n",
    "            synonyms_added = self.mine_synonyms(train_queries)\n",
    "            \n",
    "            # Step 3: Optimize hybrid weights\n",
    "            optimization_result = self.optimize_hybrid_weights(train_queries)\n",
    "            \n",
    "            # Step 4: Apply optimization with acceptance policy\n",
    "            accepted = self.apply_optimization(optimization_result)\n",
    "            \n",
    "            # Record iteration results\n",
    "            iteration_result = {\n",
    "                'iteration': iteration + 1,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'metrics': current_metrics,\n",
    "                'optimization': optimization_result,\n",
    "                'synonyms_added': synonyms_added,\n",
    "                'accepted': accepted\n",
    "            }\n",
    "            \n",
    "            self.history.append(iteration_result)\n",
    "            \n",
    "            # Save progress\n",
    "            self.save_progress()\n",
    "            \n",
    "            print(f\"   ðŸ“ˆ Iteration {iteration + 1} complete\")\n",
    "            \n",
    "            # Small delay between iterations\n",
    "            time.sleep(1)\n",
    "        \n",
    "        print(\"\\nðŸŽ‰ Self-improvement loop completed!\")\n",
    "        return self.history\n",
    "    \n",
    "    def save_progress(self):\n",
    "        \"\"\"Save improvement loop progress.\"\"\"\n",
    "        progress_file = f\"runs/improvement_loop_{int(time.time())}.json\"\n",
    "        with open(progress_file, 'w') as f:\n",
    "            json.dump({\n",
    "                'history': self.history,\n",
    "                'final_config': {\n",
    "                    'bm25_weight': self.current_config.bm25_weight,\n",
    "                    'dense_weight': self.current_config.dense_weight\n",
    "                }\n",
    "            }, f, indent=2)\n",
    "        \n",
    "        print(f\"   ðŸ’¾ Progress saved to {progress_file}\")\n",
    "    \n",
    "    def get_improvement_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get summary of improvement over iterations.\"\"\"\n",
    "        if not self.history:\n",
    "            return {}\n",
    "        \n",
    "        # Extract metrics over time\n",
    "        iterations = [h['iteration'] for h in self.history]\n",
    "        em_scores = [h['metrics']['em'] for h in self.history]\n",
    "        f1_scores = [h['metrics']['f1'] for h in self.history]\n",
    "        diversity_scores = [h['metrics']['diversity'] for h in self.history]\n",
    "        \n",
    "        return {\n",
    "            'iterations': iterations,\n",
    "            'em_scores': em_scores,\n",
    "            'f1_scores': f1_scores,\n",
    "            'diversity_scores': diversity_scores,\n",
    "            'total_synonyms': sum(h['synonyms_added'] for h in self.history),\n",
    "            'accepted_optimizations': sum(h['accepted'] for h in self.history)\n",
    "        }\n",
    "\n",
    "print(\"SelfImprovingRAG class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62ae54d",
   "metadata": {},
   "source": [
    "## Running the Self-Improvement Loop\n",
    "\n",
    "Now let's run the self-improvement loop and observe how the system optimizes itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74948be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the self-improving RAG system\n",
    "rag_system = SelfImprovingRAG(CORPUS, max_iterations=3)  # Use fewer iterations for demo\n",
    "\n",
    "# Run the improvement loop\n",
    "history = rag_system.run_improvement_loop(TRAIN_QUERIES, EVAL_QUERIES)\n",
    "\n",
    "print(\"\\nðŸ“Š Improvement Summary:\")\n",
    "summary = rag_system.get_improvement_summary()\n",
    "print(f\"Total iterations: {len(summary.get('iterations', []))}\")\n",
    "print(f\"Synonyms added: {summary.get('total_synonyms', 0)}\")\n",
    "print(f\"Optimizations accepted: {summary.get('accepted_optimizations', 0)}\")\n",
    "\n",
    "if summary.get('em_scores'):\n",
    "    print(f\"EM improvement: {summary['em_scores'][0]:.3f} â†’ {summary['em_scores'][-1]:.3f}\")\n",
    "    print(f\"F1 improvement: {summary['f1_scores'][0]:.3f} â†’ {summary['f1_scores'][-1]:.3f}\")\n",
    "    print(f\"Diversity improvement: {summary['diversity_scores'][0]:.3f} â†’ {summary['diversity_scores'][-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa3b7fe",
   "metadata": {},
   "source": [
    "## Visualizing Improvement Over Time\n",
    "\n",
    "Let's create visualizations to show how the system improved over iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33034bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create improvement visualization\n",
    "if summary:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Self-Improvement Loop Results', fontsize=16)\n",
    "    \n",
    "    iterations = summary.get('iterations', [])\n",
    "    \n",
    "    # EM and F1 scores over time\n",
    "    if summary.get('em_scores'):\n",
    "        axes[0,0].plot(iterations, summary['em_scores'], 'o-', label='EM Score', linewidth=2)\n",
    "        axes[0,0].plot(iterations, summary['f1_scores'], 's-', label='F1 Score', linewidth=2)\n",
    "        axes[0,0].set_title('Retrieval Performance Over Time')\n",
    "        axes[0,0].set_xlabel('Iteration')\n",
    "        axes[0,0].set_ylabel('Score')\n",
    "        axes[0,0].legend()\n",
    "        axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Diversity over time\n",
    "    if summary.get('diversity_scores'):\n",
    "        axes[0,1].plot(iterations, summary['diversity_scores'], '^-', \n",
    "                       color='orange', linewidth=2)\n",
    "        axes[0,1].set_title('Retriever Diversity Over Time')\n",
    "        axes[0,1].set_xlabel('Iteration')\n",
    "        axes[0,1].set_ylabel('Average Jaccard Similarity')\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Optimization acceptance\n",
    "    accepted = [h['accepted'] for h in history]\n",
    "    axes[1,0].bar(iterations, accepted, color=['green' if a else 'red' for a in accepted])\n",
    "    axes[1,0].set_title('Optimization Acceptance')\n",
    "    axes[1,0].set_xlabel('Iteration')\n",
    "    axes[1,0].set_ylabel('Accepted (1) / Rejected (0)')\n",
    "    axes[1,0].set_yticks([0, 1])\n",
    "    \n",
    "    # Synonyms added per iteration\n",
    "    synonyms = [h['synonyms_added'] for h in history]\n",
    "    axes[1,1].bar(iterations, synonyms, color='purple', alpha=0.7)\n",
    "    axes[1,1].set_title('Synonyms Added per Iteration')\n",
    "    axes[1,1].set_xlabel('Iteration')\n",
    "    axes[1,1].set_ylabel('Number of Synonyms')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed results\n",
    "    print(\"\\nðŸ“‹ Detailed Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    for i, result in enumerate(history):\n",
    "        print(f\"\\nIteration {result['iteration']}:\")\n",
    "        print(f\"  EM: {result['metrics']['em']:.3f}\")\n",
    "        print(f\"  F1: {result['metrics']['f1']:.3f}\")\n",
    "        print(f\"  Diversity: {result['metrics']['diversity']:.3f}\")\n",
    "        print(f\"  Synonyms added: {result['synonyms_added']}\")\n",
    "        print(f\"  Optimization accepted: {result['accepted']}\")\n",
    "        print(f\"  Run ID: {result['metrics']['run_id']}\")\n",
    "else:\n",
    "    print(\"No improvement data available to visualize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a619da",
   "metadata": {},
   "source": [
    "## Testing the Improved System\n",
    "\n",
    "Let's test the final optimized system on some queries to see the improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eb1eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the optimized system\n",
    "print(\"ðŸ§ª Testing Optimized System\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Load the final optimized configuration\n",
    "try:\n",
    "    final_config = load_hybrid_config()\n",
    "    print(f\"Final configuration - BM25: {final_config.bm25_weight:.3f}, \"\n",
    "          f\"Dense: {final_config.dense_weight:.3f}\")\n",
    "except:\n",
    "    print(\"Using default configuration\")\n",
    "    final_config = None\n",
    "\n",
    "# Test on evaluation queries\n",
    "test_results = []\n",
    "for query in EVAL_QUERIES[:3]:  # Test on first 3 queries\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Get results from optimized hybrid retriever\n",
    "    results = hybrid.hybrid_retrieve(query, CORPUS, 3)\n",
    "    \n",
    "    print(\"Top 3 results:\")\n",
    "    for i, doc in enumerate(results):\n",
    "        print(f\"  {i+1}. {doc}\")\n",
    "    \n",
    "    test_results.append({\n",
    "        'query': query,\n",
    "        'results': results\n",
    "    })\n",
    "\n",
    "print(\"\\nâœ… Testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2f1495",
   "metadata": {},
   "source": [
    "## Key Insights from Self-Improvement\n",
    "\n",
    "From running the self-improvement loop, we can observe:\n",
    "\n",
    "1. **Automatic Optimization**: The system automatically finds better hybrid weights\n",
    "2. **Acceptance Policy**: Only accepts improvements that meet quality thresholds\n",
    "3. **Synonym Mining**: Learns from retriever disagreements to improve retrieval\n",
    "4. **Iterative Refinement**: Performance improves over multiple iterations\n",
    "5. **Robustness**: System maintains stability even when optimizations are rejected\n",
    "\n",
    "## Real-World Applications\n",
    "\n",
    "This self-improvement approach can be applied to:\n",
    "- **Production RAG Systems**: Continuous optimization in live environments\n",
    "- **Research**: Automated hyperparameter tuning and architecture search\n",
    "- **Quality Assurance**: Ensuring retrieval quality meets standards\n",
    "- **A/B Testing**: Comparing different retrieval strategies automatically\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Scale to larger datasets and more complex queries\n",
    "- Add more optimization dimensions (beyond just weights)\n",
    "- Implement more sophisticated acceptance policies\n",
    "- Add human-in-the-loop validation\n",
    "- Deploy in production with monitoring and alerting\n",
    "\n",
    "This notebook demonstrates the core self-improvement capabilities of the `autorag-live` system, showing how RAG pipelines can automatically optimize themselves through iterative refinement and disagreement analysis."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
